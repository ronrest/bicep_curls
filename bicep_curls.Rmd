---
title: "Bicep Curls"
author: "Ronny Restrepo"
date: "21/05/2015"
output: html_document
---


```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
# make this an external chunk that can be included in any file
library(knitr)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, 
               fig.align = 'center', dpi = 100, tidy = F, 
               cache.path = '.cache/', cache=TRUE, 
               fig.path = 'fig/', fig.height=3, fig.width=4)

options(xtable.type = 'html')

knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```



# TIPS
- `readRDS()` and `saveRDS()` for caching R objects such as the trained data. 
-  use variable Importance `varImp()` function on the trained model. It shows the most important variables. 
- Use `myModel$results` for a summary of accuracy results

```{r conveniencelibs}
#Uncomment the following lines if you want to install 
#   file.convenience package
#   stat.convenience package
#   fancyprint       package
#library(devtools)
#TODO: XXXX install from github code

#library(file.convenience)
#library(stat.convenience)

```

```{r getData, dependson=conveniencelibs}
#===============================================================================
#                                                        DOWNLOAD AND CACHE DATA
#===============================================================================
trainURL = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
predictURL = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

library(file.convenience)        # Not in CRAN, see Appendix to get this package
cacheDownload(trainURL, dataDir="data", localName="trainData")
cacheDownload(predictURL, dataDir="data", localName="predictionData")
```


``` {r loadData, dependson="getData"}
#===============================================================================
#                                                                      LOAD DATA
#===============================================================================
na.strings = c("NA", "#DIV/0!")
rawData = read.csv("data/trainData", na.strings=na.strings, stringsAsFactors=FALSE)
predictData = read.csv("data/predictionData", na.strings=na.strings, 
                   stringsAsFactors=FALSE)
```

```{r summary, dependson="loadData"}
#===============================================================================
#                                                                       SUMMARY
#===============================================================================
# TODO: remove the detach line
#detach("package:stat.convenience", unload=TRUE)
library(fancyprint)              # Not in CRAN, see Appendix to get this package
library(stat.convenience)        # Not in CRAN, see Appendix to get this package
na.info = na.summary(rawData, only.nas=TRUE)
nrow(na.info)

#na.summary(test, only.nas=TRUE)
```

We can see from the the summary of NAs, that there are 100 columns with NAs. Out 
of those columns, the proportion of NAs range from  
`r round(min(na.info$proportion) * 100, digits=2)`% to 
`r round(max(na.info$proportion) * 100, digits=2)`% of the rows. This means it 
is not really worth keeping those columns in the dataset, so we keep a subset of 
the columns that dont have any NAs. 

There are some aditional columns that are not much use to us as predictor 
variables, so they are filtered also. 

```{r filterColumns, dependson="loadData"}
#===============================================================================
#                                                                 FILTER COLUMNS
#===============================================================================
# Create filter of columns with the NAs
column_filter <- na.info$colName

# Filter includes aditional columns that are not useful for prediction
column_filter <- c(column_filter, "X", "user_name", "raw_timestamp_part_1", 
                   "raw_timestamp_part_2", "cvtd_timestamp", "new_window", 
                   "num_window")

# Actually filter out the columns using the filter
# filter.columns() is in the stat.convenience package
cleanData <- filter.columns(rawData, column_filter, method="list", exclude=TRUE)
predictData <- filter.columns(predictData, column_filter, method="list", exclude=TRUE)

# Convert the class column to factor type
cleanData$classe <- as.factor(cleanData$classe)
```

What we end up with is `r ncol(cleanData)` columns. `r ncol(cleanData) - 1` of 
them to be used as predictor variables, and the column labelled `classe` 
provides the labels to be used in training the learning algorithm. 

Now that the data has been cleaned up a bit, we can split the data into a 
training and test set. 60% of the data is assigned to the training set, and 40% 
to the test set. 



```{r splitData}
#===============================================================================
#                                                                     SPLIT DATA
#===============================================================================
library(caret)
set.seed(974)
inTrain <- createDataPartition(y=cleanData$classe, p=0.6, list=FALSE)
trainData <- cleanData[inTrain,]
testData <- cleanData[-inTrain,]
```

Now we can train the learning algorithm. 

```{r}
#===============================================================================
#                                                                     TRAIN DATA
#===============================================================================
#Uncomment to set number of cores in Revolution R
#library(RevoUtilsMath)
#setMKLthreads(2)

#install.packages("doParallel")
library(doParallel)




################################################################################
#                                                                      FINALISTS
################################################################################
#=========================================================================
#                                    gbm, center&scale, repeatedcv n10, r3
#                                                   TRYING NOW ON TESTDATA                                                                      
library("gbm")
registerDoParallel(cores=2)
# cores=8     miniTrain  elapsed=   29.840
# cores=2     miniTrain2 elapsed=   59.200
# cores=2     trainData  elapsed=   782.468

system.time({
    tc <- trainControl(method="repeatedcv", number=10, repeats=3)
    modFit3 <- train(classe ~ ., method="gbm", verbose=FALSE,
                    preProcess=c("center","scale"), trControl=tc, 
                    data=trainData)
})
print(modFit3)
# miniTrain    preprocess= center,scale  Accuracy   0.7240634 - 0.9009502
# miniTrain2   preprocess= center,scale  Accuracy   0.7498513 - 0.9289771
# trainData    preprocess= center,scale  Accuracy   0.7544986 - 0.9606821
#saveRDS(modFit3, "modFit_gbm_center_scale_repeatedCV_n1-_r3_trainData.rds")
#=========================================================================

#=========================================================================
#                        Random Forrest, no preprocess, repeatedcv n10, r3 
#                                                              PRETTY GOOD
registerDoParallel(cores=2)
# cores=8     miniTrain  elapsed=   72.190
# cores=8     miniTrain2 elapsed=  194.933
# cores=8     miniTrain4 elapsed=  269.738      with n=5
# cores=8     miniTrain4 elapsed=  652.625      with n=10
# cores=2     trainData  elapsed=  6756.159     


system.time({
    tc <- trainControl(method="repeatedcv", number=10, repeats=3)
    modFit3 <- train(classe ~ ., method="rf", prox=TRUE,  trControl=tc, 
                     data=trainData)
})
print(modFit3)
# miniTrain    preprocess= NA  Accuracy   0.9053133  - 0.9078660
# miniTrain2   preprocess= NA  Accuracy   0.9468010  - 0.9469440
# miniTrain4   preprocess= NA  Accuracy   0.9643448  - 0.9695097  with n=5
# miniTrain4   preprocess= NA  Accuracy   0.9687342  - 0.9743236  with n=10
# trainData    preprocess= NA  Accuracy   0.9856776  - 0.9907160  kappa=0.9882559

#saveRDS(modFit3, "modFit_rf_noPreproc_repeatedCv_n10_r3_trainData.rds")
#=========================================================================

#=========================================================================
#                      Random Forrest, center&scale, repeatedcv n10, r3
registerDoParallel(cores=2)
# cores=8     miniTrain  elapsed=   77.471
# cores=8     miniTrain2 elapsed=  203.801
# cores=2     trainData  elapsed=  5855.466
# 
set.seed(473)
system.time({
    tc <- trainControl(method="repeatedcv", number=10, repeats=3)
    modFit3 <- train(classe ~ ., method="rf", prox=TRUE, 
                     preProcess=c("center","scale"), trControl=tc, 
                     data=trainData)
})
print(modFit3)
# miniTrain    preprocess= center,scale  Accuracy   0.8996508  - 0.9089277
# miniTrain2   preprocess= center,scale  Accuracy   0.9470743  - 0.9548542
# trainData   preprocess= center,scale  Accuracy   0.9844323  - 0.9905175

#saveRDS(modFit3, "modFit_rf_centreScale_repeatedcv_n10_r3_trainData.rds")
#=========================================================================





```

```{r}
#===============================================================================
#                                                                 EXPORT ANSWERS
#===============================================================================
#answers = rep("A", 20)
#answers = c("A", "B", "A", "C", "A", "B", "A", "C", "A")

#then you can load this function by copying and pasting it into R: 
# pml_write_files = function(x){
#   n = length(x)
#   for(i in 1:n){
#     filename = paste0("answers/","problem_id_",i,".txt")
#     write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
#   }
# }
# #then create a folder where you want the files to be written. 
# # Set that to be your working directory and run:
#  
# pml_write_files(answers)
```


"R version 3.2.0 (2015-04-16)"
