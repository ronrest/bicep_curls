---
title: "Qualitative Activity Recognition of Bicep Curls"
author: "Ronny Restrepo"
date: "21/05/2015"
output: html_document
---


```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
# make this an external chunk that can be included in any file
library(knitr)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, 
               fig.align = 'center', dpi = 100, tidy = F, 
               cache.path = '.cache/', cache=TRUE, 
               fig.path = 'fig/', fig.height=3, fig.width=4)

options(xtable.type = 'html')

knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

# Introduction
The increasing affordability and miniaturization of sensors such as 
acceleromenters, gyroscopes, magnetometers and heart rate monitors is making it 
increasingly possible for people to be able to record detailed information about 
their personal activities such as sleep patterns, estimated calories burnt, 
distances run, how many steps have been taken, etc. This has caught on as a 
social movement known as the _quantified self movement_. Commercial devices such 
as Fitbit, Jawbone Up, and Nike FuelBand have popularised this trend, making it 
increasingly easy to record and analyse data on such activities. 

Given that different activities require different interactions of movements 
between different body parts, such technology potentially lends itself to also 
being able to analyse whether particular activities are being performed 
correctly. This is known as _Qualitative Activity Recognition_ (QAR). 

This paper looks at the kind of accuracy that can be achieved in detecting if a 
person is performing a bicep curl correctly, or if they are performing one of the 
common mistakes. It aims to do so by using data from multiple sensors on the 
body, and equipment to make this prediction based on a model trained by a 
machine learning algorithm. 

# Data
The data used is taken from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv which is a subset of the Weight Lifting Exercises Dataset by Velloso et al (2013) found  at http://groupware.les.inf.puc-rio.br/har . 

The dataset contains 19622 rows of observations from sensors located on the 
forearm, arm, belt, and dumbell for 6 male participants between the ages of 
20-28 years performing bicep curls. All participants had weight lifting 
experience and were instructed to perform bicep curls in 5 different ways. One 
of those ways was the correct way of performing dumbell curls (Class A). The 
other four ways were dumbell curls performed in such a way to mimick common 
mistakes such as performing the dumbell curl by "throwing the elbows to the 
front (Class B), lifting the dumbbell only halfway (Class C), lowering the 
dumbbell only halfway (Class D) and throwing the hips to the front (Class E)" 
(_Velloso et al_, 2013).

# Method

For the purposes of this analysis, R is used for the entire pipeline, from raw 
data to clean data, to the creation of the trained model, and for prediction of 
new data. All code is provided in the steps below. 

## Download the Data

The first step is to download the data. 

```{r getData}
#===============================================================================
#                                                        DOWNLOAD AND CACHE DATA
#===============================================================================
trainURL = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"

# file.convenience package is not in CRAN, see Appendix A to get this package
library(file.convenience)
cacheDownload(trainURL, dataDir="data", localName="trainData")
```

Next, is to load the raw data into the R session. 

``` {r loadData, dependson="getData"}
#===============================================================================
#                                                                      LOAD DATA
#===============================================================================
na.strings = c("NA", "#DIV/0!")    # missing values stored as "NA" and "#DIV/0!"
rawData = read.csv("data/trainData", na.strings=na.strings, stringsAsFactors=F)
```

## Clean Up the Data
We look at the data for an missing values. 

```{r summary, dependson="loadData"}
#===============================================================================
#                                                      SUMMARY OF MISSING VALUES
#===============================================================================
library(fancyprint)            # Not in CRAN, see Appendix A to get this package
library(stat.convenience)      # Not in CRAN, see Appendix A to get this package
na.info = na.summary(rawData, only.nas=TRUE, printit=FALSE)
length(na.info$proportion)     # Number of columns with NAs
min(na.info$proportion)        # Min Proportion of NAs in columns
max(na.info$proportion)        # Max proportion of NAs in columns
```


A full printout of the `na.summary()` function call can be seen in Appendix B. 
We can see from the the summary of information regarding missing values that 
there are 100 columns with missing data. Out of these columns with missing 
values, at least `r round(min(na.info$proportion) * 100, digits=2)`% of the 
values are missing, and for some columns, 
`r round(max(na.info$proportion) * 100, digits=2)`% of the data is missing. This 
means it is not really worth keeping any of those columns in the dataset, so we 
create a subset of the columns that dont have any NAs. 

There are also some aditional aditional columns that are not much use to us as 
predictor variables, so they are also filtered out. 

```{r filterColumns, dependson="loadData"}
#===============================================================================
#                                                                 FILTER COLUMNS
#===============================================================================
# Create filter of columns with the NAs
column_filter <- na.info$colName

# Filter includes aditional columns that are not useful for prediction
column_filter <- c(column_filter, "X", "user_name", "raw_timestamp_part_1", 
                   "raw_timestamp_part_2", "cvtd_timestamp", "new_window", 
                   "num_window")

# Actually filter out the columns using the filter
# filter.columns() is in the stat.convenience package
cleanData <- filter.columns(rawData, column_filter, method="list", exclude=TRUE)

# Convert the class column to factor type
cleanData$classe <- as.factor(cleanData$classe)
```

What we end up with is `r ncol(cleanData)` columns. `r ncol(cleanData) - 1` of 
them to be used as predictor variables, and the column labelled `classe` 
provides the labels to be used in training the learning algorithm. 


## Train the Machine Learning Algorithm

Now that the data has been cleaned up a bit, we can split the data into a 
training and test set for the learning algorithm. 60% of the data is assigned to 
the training set, and 40% to the test set. 

```{r splitData, dependson="filterColumns"}
#===============================================================================
#                                                                     SPLIT DATA
#===============================================================================
library(e1071)
library(caret)
set.seed(974)
inTrain <- createDataPartition(y=cleanData$classe, p=0.6, list=FALSE)
trainData <- cleanData[inTrain,]
testData <- cleanData[-inTrain,]
```

Now we can train the machine learning algorithm. A Random Forrest is used with 
three separate 10-fold cross-validations. Note that this training process may 
take a few hours to run. Also note that the code below has been configured to 
run with parallel processing, using 2 threads on a multi-core processor. Given 8 
Gigabytes of RAM, and given the sze of this training set, this was about as many 
threads that could be used without overflowing RAM and spilling into Swap memory. 

```{r train, dependson="splitData"}
#===============================================================================
#                                                                     TRAIN DATA
#===============================================================================
#-------------------------------------------------------------------------
#                                                      Parallel Processing 
#-------------------------------------------------------------------------
numThreads = 2
#Uncomment to set number of cores in Revolution R
#library(RevoUtilsMath)
#setMKLthreads(numThreads)

#install.packages("doParallel")
library(doParallel)
registerDoParallel(cores=2)

#-------------------------------------------------------------------------
#                        Random Forrest, no preprocess, repeatedcv n10, r3 
#-------------------------------------------------------------------------
# Cache the trained model in a subdirectory
modelCacheDir = "trained_objects"
modelCache = "trained_objects/modFit_rf_noPreproc_repeatedCv_n10_r3_trainData.rds"

if(!file.exists(modelCacheDir)){
    dir.create(modelCacheDir)
}
if(!file.exists(modelCache)){
    set.seed(473)
    tc <- trainControl(method="repeatedcv", number=10, repeats=3)
    trainedModel <- train(classe ~ ., method="rf", prox=TRUE,  trControl=tc, 
                      data=trainData)
    saveRDS(trainedModel, modelCache)
}else{
    trainedModel = readRDS(modelCache)
}
```


## Summary of Model
The training process performed 10-fold cross validation on three separate models. 
The model  with the greatest accuracy achieved an estimated accuracy of 99.07% 
and estimated Kappa of 98.83%. 

``` {r trainSummaryMini, dependson="train"}
trainedModel$results
```

See _Appendix C_ for a more complete printout summary of the different models. 

The three most important variables in predicting the categories are `roll_belt`, 
`pitch_forearm` and `yaw_belt`. The printout of the 20 most important variables 
can be seen in _Appendix D_.


## Testing the Model

K-fold cross validation does a good job at predicting the out of sample accuracy 
but, it is always best to test the model with completely new data that it has not 
encountered just to ensure it has not overfitted to the training set. Previously, 
40% of the data was set asside as a test set. This will be used to test how well 
our trained model actually does on new data. 

```{r test, dependson="train"}
#===============================================================================
#                                                        APPLY MODEL TO TEST SET
#===============================================================================
pred <- predict(trainedModel, testData) 
```

The confusion matrix below tells us that the predicted out of sample accuracy for 
our trained model is indeed was indeed fairly accurate. The model had predicted 
an accuracy of 99.07% and estimated Kappa of 98.83%. On the new data we get an 
accuracy of 99.1%, and kappa of 98.86%. 

```{r confuseMatrix, dependson="test"}
confusionMatrix(pred, testData$classe)
```


# Conclusion

The trained model performs very well however, the data is only from 6 
participants, all of which have had weight lifting experience. It would be 
good to see larger amount of data recorded for a wider set of participants at 
different skill levels to both evaluate the true accuracy of such a model, or 
to train it further so that it generalises to the general population. 

A potential application of this kind of Qualitative Activity Recognition could 
be as a training tool. For instance, someone that is 
learning some new exercise could receive instantaneous customised feedback 
regarding how well their form is, and what adjustments could be made to improve. 


# References
- Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.


# Appendix A - Installing Convenience Packages

The `file.convenience`, `stat.convenience` and `fancyprint` packages are not on 
the CRAN repository, so if you want to install them you will need to run the 
following code: 

```r
# Requires the devtools package to install packages from GitHub
#install.packages("devtools")
library(devtools)

# Install convenience functions from Github
install_github("ronrest/fancyprint_R/fancyprint")
install_github("ronrest/convenience_functions_R/stat.convenience")
install_github("ronrest/convenience_functions_R/file.convenience")
```

# Appendix B - Output of na.summary()

Below is a full printout of the summary of NAs for each column in the raw data. 

```{r nainfo, dependson="summary"}
na.summary(rawData, only.nas=TRUE)
```


# Appendix C - Summary of Trained Models

```{r summaryModel, dependson="train"}
print(trainedModel)
```

# Appendix D - 20 Most Important Predictive Variables

```{r varImp, dependson="train"}
# relative importance of different variables
varImp(trainedModel)
```

